# Go pprof Debugging Guide for Production

**Date:** 2026-02-14
**Endpoints:** `/debug/pprof/*` (added in commit `f82abbc6`)

---

## Quick Reference

```bash
# CPU profiling (30 seconds)
curl http://localhost:8080/debug/pprof/profile > cpu.prof
go tool pprof cpu.prof

# Memory profiling
curl http://localhost:8080/debug/pprof/heap > heap.prof
go tool pprof heap.prof

# Goroutine leak detection
curl http://localhost:8080/debug/pprof/goroutine > goroutine.prof
go tool pprof goroutine.prof

# Interactive web UI
go tool pprof -http=:6060 http://localhost:8080/debug/pprof/profile
```

---

## Common Debugging Scenarios

### Scenario 1: High CPU Usage

**Symptom:** Server CPU at 80-100%

**Debug:**
```bash
# Capture 30-second CPU profile
curl http://localhost:8080/debug/pprof/profile?seconds=30 > cpu.prof

# Analyze with pprof
go tool pprof cpu.prof

# In pprof interactive mode:
(pprof) top10          # Show top 10 CPU consumers
(pprof) list FunctionName  # Show source code with CPU samples
(pprof) web            # Open flamegraph in browser
```

**Look for:**
- JSON marshaling/unmarshaling hotspots
- Database query functions
- HTTP request handlers
- Regex compilation in loops

**Real Example:**
```
(pprof) top10
Showing nodes accounting for 2.5s, 83.33% of 3.0s total
      flat  flat%   sum%        cum   cum%
     0.8s 26.67% 26.67%      0.8s 26.67%  encoding/json.(*encodeState).marshal
     0.5s 16.67% 43.33%      0.5s 16.67%  runtime.scanobject
     0.4s 13.33% 56.67%      0.4s 13.33%  internal/adapters/rubicon.MakeRequests
     0.3s 10.00% 66.67%      1.2s 40.00%  net/http.(*conn).serve
```

**Fix:** If JSON marshal is hot, consider caching marshaled responses or pre-computing.

---

### Scenario 2: Memory Leak

**Symptom:** Memory usage grows over time, never released

**Debug:**
```bash
# Capture heap snapshot
curl http://localhost:8080/debug/pprof/heap > heap.prof

# Analyze
go tool pprof heap.prof

(pprof) top10          # Top memory allocators
(pprof) list FunctionName
(pprof) peek FunctionName  # Show call stack
```

**Look for:**
- Large slice/map allocations
- Goroutines holding references
- Unclosed HTTP response bodies
- Growing caches without eviction

**Real Example:**
```
(pprof) top10 -inuse_space
Showing nodes accounting for 512MB, 95% of 540MB total
      flat  flat%   sum%        cum   cum%
   256MB 47.41% 47.41%    256MB 47.41%  bytes.makeSlice
   128MB 23.70% 71.11%    128MB 23.70%  encoding/json.Unmarshal
    64MB 11.85% 82.96%     64MB 11.85%  internal/storage.IDGraphStore.cache
```

**Fix:** If cache is large, implement size limits or TTL-based eviction.

---

### Scenario 3: Goroutine Leak

**Symptom:** Goroutine count keeps growing (check `/metrics`)

**Debug:**
```bash
# Capture goroutine profile
curl http://localhost:8080/debug/pprof/goroutine > goroutine.prof

go tool pprof goroutine.prof

(pprof) top10          # Count by function
(pprof) traces         # Show full stack traces
(pprof) list FunctionName
```

**Look for:**
- HTTP handlers waiting on channels
- Database queries not timing out
- Infinite loops
- Blocked channel operations

**Real Example:**
```
(pprof) top10
Showing goroutines accounting for 1250, 98.42% of 1270 total
      flat  flat%   sum%        cum   cum%
      500 39.37% 39.37%      500 39.37%  internal/pkg/redis.(*Client).Subscribe
      400 31.50% 70.87%      400 31.50%  net/http.(*persistConn).writeLoop
      200 15.75% 86.61%      200 15.75%  internal/exchange.(*Exchange).processTimeout
```

**Fix:** Check for goroutines started but never stopped, add context timeouts.

---

### Scenario 4: Slow Requests

**Symptom:** Requests take 2-5 seconds instead of 100-200ms

**Debug:**
```bash
# Capture trace during slow period (careful: large file)
curl http://localhost:8080/debug/pprof/trace?seconds=5 > trace.out

# View in trace viewer
go tool trace trace.out
```

**What you'll see:**
- Timeline of all goroutines
- HTTP request latency breakdown
- Scheduler events
- GC pauses
- Lock contention

**Look for:**
- Long blocks waiting for mutex
- GC pauses > 10ms
- Network I/O waiting
- Database query time

**Real Example:**
In trace viewer, you might see:
- HTTP handler starts at 0ms
- Waits for database from 10ms-1500ms (slow query!)
- JSON marshal from 1500ms-1550ms
- Response written at 1560ms

**Fix:** Optimize the slow query, add index, or cache results.

---

## Production Debugging Workflow

### Step 1: Identify the Problem

```bash
# Check Prometheus metrics
curl http://localhost:8080/metrics | grep -E "(cpu|memory|goroutines|latency)"

# Look for:
# - process_resident_memory_bytes increasing
# - go_goroutines increasing
# - http_request_duration_seconds_bucket high values
```

### Step 2: Capture Profile

```bash
# For CPU issues
curl http://localhost:8080/debug/pprof/profile?seconds=30 > cpu-$(date +%s).prof

# For memory issues
curl http://localhost:8080/debug/pprof/heap > heap-$(date +%s).prof

# For goroutine leaks
curl http://localhost:8080/debug/pprof/goroutine > goroutine-$(date +%s).prof

# For detailed timeline (use sparingly - expensive)
curl http://localhost:8080/debug/pprof/trace?seconds=5 > trace-$(date +%s).out
```

### Step 3: Analyze Locally

```bash
# Copy profile from server
scp catalyst:/tmp/cpu-*.prof .

# Analyze with web UI (easiest)
go tool pprof -http=:6060 cpu-*.prof

# Or interactive mode
go tool pprof cpu-*.prof
```

### Step 4: Compare Before/After

```bash
# Capture baseline during normal operation
curl http://localhost:8080/debug/pprof/heap > heap-baseline.prof

# Capture during high memory usage
curl http://localhost:8080/debug/pprof/heap > heap-problem.prof

# Compare to see what changed
go tool pprof -base=heap-baseline.prof heap-problem.prof
```

---

## Web UI Features (Recommended)

The web UI (`go tool pprof -http=:6060`) provides:

### 1. Flamegraph
Shows call stack hierarchy - wider bars = more CPU/memory

**How to read:**
- Width = percentage of total
- Depth = call stack depth
- Hover for exact values
- Click to zoom into function

### 2. Top Table
Lists functions by flat/cumulative CPU or memory

**Columns:**
- **Flat:** Time/memory in this function only
- **Cum:** Time/memory in this function + callees
- **Sum%:** Cumulative percentage

### 3. Graph View
Visual call graph with percentages

**Useful for:**
- Understanding call relationships
- Finding bottlenecks
- Identifying hot paths

### 4. Source Code View
Shows source with annotations

**Useful for:**
- Seeing which lines are hot
- Understanding context
- Planning optimizations

---

## Real-World Examples

### Example 1: JSON Marshal Hotspot

**Profile showed:**
```
flat=800ms, cum=800ms in encoding/json.(*encodeState).marshal
```

**Investigation:**
```go
// In bid handler
for _, bid := range bids {
    jsonBytes, _ := json.Marshal(bid)  // ‚Üê Hot!
    // ...
}
```

**Fix:**
```go
// Pre-allocate encoder, reuse buffer
var buf bytes.Buffer
encoder := json.NewEncoder(&buf)
for _, bid := range bids {
    buf.Reset()
    encoder.Encode(bid)  // ‚Üê 40% faster
    // ...
}
```

---

### Example 2: Goroutine Leak in Redis Subscriber

**Profile showed:**
```
1000 goroutines stuck in redis.(*Client).Subscribe
```

**Investigation:**
```go
// In ID graph sync
go func() {
    client.Subscribe(ctx, "id-updates")  // ‚Üê Never closes!
}()
```

**Fix:**
```go
go func() {
    defer client.Close()
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
    defer cancel()
    client.Subscribe(ctx, "id-updates")
}()
```

---

### Example 3: Memory Leak in Impression Cache

**Profile showed:**
```
512MB in internal/storage.(*BidderStore).impressionCache
```

**Investigation:**
```go
// Cache grows unbounded
type BidderStore struct {
    impressionCache map[string]*Impression  // ‚Üê Never evicts!
}
```

**Fix:**
```go
// Add LRU cache with max size
import "github.com/hashicorp/golang-lru"

type BidderStore struct {
    impressionCache *lru.Cache  // Max 10000 entries
}

func NewBidderStore() *BidderStore {
    cache, _ := lru.New(10000)
    return &BidderStore{impressionCache: cache}
}
```

---

## Tips & Best Practices

### 1. Minimize Profiling Overhead

- CPU profiling: ~5% overhead during 30s capture
- Heap profiling: Instant snapshot, no overhead
- Trace: 10-15% overhead, use sparingly
- Goroutine: Instant snapshot, no overhead

**Best practice:** Profile during off-peak hours or on a single replica.

### 2. Compare Profiles Over Time

```bash
# Weekly baseline
curl http://localhost:8080/debug/pprof/heap > heap-weekly-$(date +%Y%m%d).prof

# Compare to last week
go tool pprof -base=heap-weekly-20260207.prof heap-weekly-20260214.prof
```

### 3. Automate Profile Collection

```bash
# Cron job: Capture profiles daily
#!/bin/bash
DATE=$(date +%Y%m%d-%H%M)
curl -s http://localhost:8080/debug/pprof/heap > /var/log/catalyst/profiles/heap-$DATE.prof
curl -s http://localhost:8080/debug/pprof/goroutine > /var/log/catalyst/profiles/goroutine-$DATE.prof

# Keep only last 30 days
find /var/log/catalyst/profiles -type f -mtime +30 -delete
```

### 4. Security Considerations

**‚ö†Ô∏è WARNING:** pprof endpoints expose sensitive runtime information.

**Production setup:**
```bash
# Restrict to internal IPs only (in firewall)
iptables -A INPUT -p tcp --dport 8080 -s 10.0.0.0/8 -j ACCEPT
iptables -A INPUT -p tcp --dport 8080 -j DROP

# Or use SSH tunnel
ssh -L 8080:localhost:8080 catalyst-server
# Then access http://localhost:8080/debug/pprof/ locally
```

---

## Integration with CATALYST

### Check Server Health

```bash
# Goroutine count (should be stable)
curl -s http://localhost:8080/debug/pprof/goroutine?debug=1 | grep "^goroutine" | wc -l

# Memory stats
curl -s http://localhost:8080/debug/pprof/heap?debug=1 | grep -A 5 "HeapAlloc"

# CPU profile sample
curl -s http://localhost:8080/debug/pprof/profile?seconds=1 > /dev/null && echo "OK"
```

### Monitor Adapter Performance

```bash
# Profile during high bid volume
curl http://localhost:8080/debug/pprof/profile?seconds=30 > cpu-high-volume.prof

# Look for adapter hotspots
go tool pprof cpu-high-volume.prof
(pprof) top10 -cum
(pprof) list internal/adapters
```

### Debug ID Graph Sync Issues

```bash
# Check goroutines in Redis subscriber
curl http://localhost:8080/debug/pprof/goroutine > goroutine.prof
go tool pprof goroutine.prof
(pprof) top10
(pprof) list internal/storage.IDGraphStore
```

---

## Cheat Sheet

| Issue | Command | What to Look For |
|-------|---------|------------------|
| High CPU | `curl .../profile > cpu.prof` | Hot functions in `top10` |
| Memory leak | `curl .../heap > heap.prof` | Growing allocations in `-inuse_space` |
| Goroutine leak | `curl .../goroutine > g.prof` | Stuck goroutines in `traces` |
| Slow requests | `curl .../trace?seconds=5 > trace.out` | Long waits in trace viewer |
| Lock contention | `curl .../mutex > mutex.prof` | High wait times in `top10` |
| GC issues | `curl .../allocs > allocs.prof` | Allocation rate |

---

## Resources

- **Go pprof docs:** https://pkg.go.dev/runtime/pprof
- **Web UI guide:** https://github.com/google/pprof/blob/main/doc/README.md
- **Flamegraph guide:** https://www.brendangregg.com/flamegraphs.html
- **Go execution tracer:** https://go.dev/doc/diagnostics#execution-tracer

---

## Summary

‚úÖ **pprof endpoints added** (commit `f82abbc6`)
‚úÖ **Available now:** `/debug/pprof/*`
‚úÖ **Zero overhead** when not profiling
‚úÖ **Production-ready** with proper security
‚úÖ **Powerful debugging** for performance issues

**Most useful for:**
- Finding CPU hotspots
- Detecting memory leaks
- Debugging goroutine leaks
- Analyzing slow requests

üîç **Start debugging with:** `go tool pprof -http=:6060 http://localhost:8080/debug/pprof/profile`
